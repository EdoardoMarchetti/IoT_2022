{"cells":[{"cell_type":"markdown","source":"### Notebook Setup","metadata":{"tags":[],"cell_id":"0044de3ef6d44fa4b3845504b6da3a51","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom glob import glob\n\n#from preprocessing import LABELS\nfrom preprocessing import *\nfrom functools import partial","metadata":{"tags":[],"cell_id":"e599572dc1e64d21a5b0f9a9eae7761e","source_hash":"9d2d08e1","execution_start":1671274494872,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#unzip dataset\n#!unzip /datasets/minispeechcommands/msc-test.zip\n#!unzip /datasets/minispeechcommands/msc-train.zip\n#!unzip /datasets/minispeechcommands/msc-val.zip","metadata":{"tags":[],"cell_id":"0970d731a7124464b8fa3e887d2ffc13","source_hash":"6acae746","execution_start":1671129355845,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_ds = tf.data.Dataset.list_files(['msc-train/go*','msc-train/stop*'])\nval_ds = tf.data.Dataset.list_files(['msc-train/go*','msc-train/stop*'])\ntest_ds = tf.data.Dataset.list_files(['msc-train/go*','msc-train/stop*'])\nlen(train_ds), len(val_ds), len(test_ds)","metadata":{"tags":[],"cell_id":"2c275d22a72a4f9a8ae9df8095ed9331","source_hash":"61ce2531","execution_start":1671274498868,"execution_millis":534,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(1600, 1600, 1600)"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### Reproducibility ","metadata":{"tags":[],"cell_id":"966de35919d94e99b6c69cee80e84995","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport random\n\nseed = 42\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nrandom.seed(seed)\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n","metadata":{"tags":[],"cell_id":"2e9e7e20615b45f6a9b66c2e8ef50481","source_hash":"32724c28","execution_start":1671274504731,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Hyperparameters","metadata":{"tags":[],"cell_id":"fa0fde858a0b41a8a4a6b2349d6374a0","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"TRAINING_ARGS = {\n    'batch_size': 20,\n    'initial_learning_rate': 0.01,\n    'end_learning_rate': 1.e-5,\n    'epochs': 20\n}\n\nLABELS = ['go','stop']\n\nPREPROCESSING_MFCCS_ARGS = {\n    'downsampling_rate': 16000,\n    'frame_length_in_s': 0.016,\n    'frame_step_in_s': 0.016,\n    'num_mel_bins' : 40,\n    'lower_frequency': 20,\n    'upper_frequency': 8000,\n    'num_mfccs_coefficients':10\n}\n\nalpha = 0.25\nfinal_sparsity = 0.7\n\nbatch_size = TRAINING_ARGS['batch_size']\nepochs = TRAINING_ARGS['epochs']","metadata":{"tags":[],"cell_id":"347b8b4620a14d1c822fd31ce81ea6ad","source_hash":"10e5fc0f","execution_start":1671274526952,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Dataset mapping","metadata":{"tags":[],"cell_id":"33e98a4ecc76498db1fdcecb943cbdd8","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"def get_mfccs_and_label(filename, downsampling_rate, frame_length_in_s, \nframe_step_in_s, num_mel_bins, lower_frequency, upper_frequency, num_mfccs_coefficients):\n\n    log_mel_spectrogram, label = get_log_mel_spectrogram(filename, \n    downsampling_rate, frame_length_in_s, frame_step_in_s, num_mel_bins, \n    lower_frequency, upper_frequency)\n    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n    mfccs = mfccs[..., :num_mfccs_coefficients]\n    \n    return mfccs, label\n\nget_frozen_mfccs = partial(get_mfccs_and_label, \n**PREPROCESSING_MFCCS_ARGS)\n\ndef preprocess_mfccs(filename):\n    signal, label = get_frozen_mfccs(filename)\n    signal = tf.expand_dims(signal, -1)\n    label_id = tf.argmax(label == LABELS)\n\n    return signal, label_id","metadata":{"tags":[],"cell_id":"2e6d821476d047ad9dc48b38269a403e","source_hash":"fdebef28","execution_start":1671274538321,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"go_stop_train_ds = train_ds.map(preprocess_mfccs).batch(batch_size).cache()\ngo_stop_val_ds = val_ds.map(preprocess_mfccs).batch(batch_size)\ngo_stop_test_ds = test_ds.map(preprocess_mfccs).batch(batch_size)\n\nprint(f'Number of batches in training set {len(go_stop_train_ds)}')\n\nfor example_batch, example_labels in go_stop_train_ds.take(1):\n  #print(example_bat\n  print('Batch Shape:', example_batch.shape)\n  print('Data Shape:', example_batch.shape[1:])\n  print('Labels:', example_labels)","metadata":{"tags":[],"cell_id":"7d0105c6faa543e6b5737ec88b0cba63","source_hash":"16c04c38","execution_start":1671274545416,"execution_millis":1924,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-17 10:55:45.707028: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n2022-12-17 10:55:45.707259: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 AVX512F FMA\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-17 10:55:46.113033: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-17 10:55:46.114589: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-17 10:55:46.114757: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-17 10:55:46.469909: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-17 10:55:46.471450: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-17 10:55:46.471634: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-17 10:55:46.800974: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-17 10:55:46.802600: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-17 10:55:46.802778: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nNumber of batches in training set 80\nBatch Shape: (20, 62, 10, 1)\nData Shape: (62, 10, 1)\nLabels: tf.Tensor([0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0], shape=(20,), dtype=int64)\n2022-12-17 10:55:47.328396: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Model","metadata":{"tags":[],"cell_id":"78d2f4d16e4946919c07ccd7d128346d","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n    tf.keras.layers.Conv2D(filters=256*alpha, kernel_size=[3, 3], strides=[2, 2],\n        use_bias=False, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=256*alpha, kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],\n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=256*alpha, kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=len(LABELS)),\n    tf.keras.layers.Softmax()\n])\n\nmodel.summary()","metadata":{"tags":[],"cell_id":"e73491835d5743569cc45e5b34b23757","source_hash":"4d1e3685","execution_start":1671274554704,"execution_millis":126,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 30, 4, 64)         576       \n                                                                 \n batch_normalization (BatchN  (None, 30, 4, 64)        256       \n ormalization)                                                   \n                                                                 \n re_lu (ReLU)                (None, 30, 4, 64)         0         \n                                                                 \n depthwise_conv2d (Depthwise  (None, 30, 4, 64)        576       \n Conv2D)                                                         \n                                                                 \n conv2d_1 (Conv2D)           (None, 30, 4, 64)         4096      \n                                                                 \n batch_normalization_1 (Batc  (None, 30, 4, 64)        256       \n hNormalization)                                                 \n                                                                 \n re_lu_1 (ReLU)              (None, 30, 4, 64)         0         \n                                                                 \n depthwise_conv2d_1 (Depthwi  (None, 30, 4, 64)        576       \n seConv2D)                                                       \n                                                                 \n conv2d_2 (Conv2D)           (None, 30, 4, 64)         4096      \n                                                                 \n batch_normalization_2 (Batc  (None, 30, 4, 64)        256       \n hNormalization)                                                 \n                                                                 \n re_lu_2 (ReLU)              (None, 30, 4, 64)         0         \n                                                                 \n global_average_pooling2d (G  (None, 64)               0         \n lobalAveragePooling2D)                                          \n                                                                 \n dense (Dense)               (None, 2)                 130       \n                                                                 \n softmax (Softmax)           (None, 2)                 0         \n                                                                 \n=================================================================\nTotal params: 10,818\nTrainable params: 10,434\nNon-trainable params: 384\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Weight  Pruning ","metadata":{"tags":[],"cell_id":"3547f9bb09ee428e97df881ac584bd4c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"import tensorflow_model_optimization as tfmot\n\nprune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\nbegin_step = int(len(go_stop_train_ds) * epochs * 0.2)\nend_step = int(len(go_stop_train_ds) * epochs)\n\npruning_params = {\n    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n        initial_sparsity=0.0,\n        final_sparsity=final_sparsity,\n        begin_step=begin_step,\n        end_step=end_step\n    )\n}\n\nmodel_for_pruning = prune_low_magnitude(model, **pruning_params)","metadata":{"tags":[],"cell_id":"34aff8d4fa774eab845bb24370379b31","source_hash":"d0b12918","execution_start":1671274752599,"execution_millis":108,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Model Training","metadata":{"tags":[],"cell_id":"b29b995aaed843fa9db433bc9b5d6313","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\ninitial_learning_rate = TRAINING_ARGS['initial_learning_rate']\nend_learning_rate = TRAINING_ARGS['end_learning_rate']\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learning_rate,\n    decay_steps=len(go_stop_train_ds) * epochs,\n)\noptimizer = tf.optimizers.Adam(learning_rate=linear_decay)\nmetrics = [tf.metrics.SparseCategoricalAccuracy()]\n\n\nmodel_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\nhistory = model_for_pruning.fit(go_stop_train_ds, \n                                epochs=epochs,\n                                validation_data=go_stop_val_ds)\n\n\n#ATTENTION, if a Graph execution error is raised, please re-run the cell","metadata":{"tags":[],"cell_id":"76f5837eb85a4c9c8b895b558c243685","source_hash":"cc792923","execution_start":1671274762417,"execution_millis":138234,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/20\n80/80 [==============================] - 9s 89ms/step - loss: 0.0858 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.0680 - val_sparse_categorical_accuracy: 0.9794\nEpoch 2/20\n80/80 [==============================] - 7s 85ms/step - loss: 0.0620 - sparse_categorical_accuracy: 0.9737 - val_loss: 0.0602 - val_sparse_categorical_accuracy: 0.9825\nEpoch 3/20\n80/80 [==============================] - 7s 84ms/step - loss: 0.0441 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.0344 - val_sparse_categorical_accuracy: 0.9869\nEpoch 4/20\n80/80 [==============================] - 7s 84ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9862 - val_loss: 0.0396 - val_sparse_categorical_accuracy: 0.9819\nEpoch 5/20\n80/80 [==============================] - 7s 85ms/step - loss: 0.0402 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0543 - val_sparse_categorical_accuracy: 0.9737\nEpoch 6/20\n80/80 [==============================] - 7s 85ms/step - loss: 0.0589 - sparse_categorical_accuracy: 0.9750 - val_loss: 0.1383 - val_sparse_categorical_accuracy: 0.9594\nEpoch 7/20\n80/80 [==============================] - 7s 84ms/step - loss: 0.0465 - sparse_categorical_accuracy: 0.9762 - val_loss: 0.0293 - val_sparse_categorical_accuracy: 0.9862\nEpoch 8/20\n80/80 [==============================] - 7s 86ms/step - loss: 0.0281 - sparse_categorical_accuracy: 0.9875 - val_loss: 0.0254 - val_sparse_categorical_accuracy: 0.9900\nEpoch 9/20\n80/80 [==============================] - 7s 87ms/step - loss: 0.0236 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.0370 - val_sparse_categorical_accuracy: 0.9881\nEpoch 10/20\n80/80 [==============================] - 7s 90ms/step - loss: 0.0255 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.0225 - val_sparse_categorical_accuracy: 0.9919\nEpoch 11/20\n80/80 [==============================] - 7s 87ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.0218 - val_sparse_categorical_accuracy: 0.9931\nEpoch 12/20\n80/80 [==============================] - 7s 85ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.0260 - val_sparse_categorical_accuracy: 0.9894\nEpoch 13/20\n80/80 [==============================] - 7s 86ms/step - loss: 0.0232 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.0305 - val_sparse_categorical_accuracy: 0.9894\nEpoch 14/20\n80/80 [==============================] - 7s 85ms/step - loss: 0.0221 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0269 - val_sparse_categorical_accuracy: 0.9906\nEpoch 15/20\n80/80 [==============================] - 7s 84ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.0197 - val_sparse_categorical_accuracy: 0.9950\nEpoch 16/20\n80/80 [==============================] - 7s 84ms/step - loss: 0.0221 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.0225 - val_sparse_categorical_accuracy: 0.9944\nEpoch 17/20\n80/80 [==============================] - 7s 85ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.0203 - val_sparse_categorical_accuracy: 0.9944\nEpoch 18/20\n80/80 [==============================] - 7s 85ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0205 - val_sparse_categorical_accuracy: 0.9937\nEpoch 19/20\n80/80 [==============================] - 7s 85ms/step - loss: 0.0173 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.0220 - val_sparse_categorical_accuracy: 0.9931\nEpoch 20/20\n80/80 [==============================] - 7s 85ms/step - loss: 0.0166 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.0178 - val_sparse_categorical_accuracy: 0.9937\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"model_for_pruning.summary()","metadata":{"tags":[],"cell_id":"a73448a8c32d4778a7ab0f0144cd97b8","source_hash":"59efc34f","execution_start":1671274950439,"execution_millis":29,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n prune_low_magnitude_conv2d   (None, 30, 4, 64)        1154      \n (PruneLowMagnitude)                                             \n                                                                 \n prune_low_magnitude_batch_n  (None, 30, 4, 64)        257       \n ormalization (PruneLowMagni                                     \n tude)                                                           \n                                                                 \n prune_low_magnitude_re_lu (  (None, 30, 4, 64)        1         \n PruneLowMagnitude)                                              \n                                                                 \n prune_low_magnitude_depthwi  (None, 30, 4, 64)        577       \n se_conv2d (PruneLowMagnitud                                     \n e)                                                              \n                                                                 \n prune_low_magnitude_conv2d_  (None, 30, 4, 64)        8194      \n 1 (PruneLowMagnitude)                                           \n                                                                 \n prune_low_magnitude_batch_n  (None, 30, 4, 64)        257       \n ormalization_1 (PruneLowMag                                     \n nitude)                                                         \n                                                                 \n prune_low_magnitude_re_lu_1  (None, 30, 4, 64)        1         \n  (PruneLowMagnitude)                                            \n                                                                 \n prune_low_magnitude_depthwi  (None, 30, 4, 64)        577       \n se_conv2d_1 (PruneLowMagnit                                     \n ude)                                                            \n                                                                 \n prune_low_magnitude_conv2d_  (None, 30, 4, 64)        8194      \n 2 (PruneLowMagnitude)                                           \n                                                                 \n prune_low_magnitude_batch_n  (None, 30, 4, 64)        257       \n ormalization_2 (PruneLowMag                                     \n nitude)                                                         \n                                                                 \n prune_low_magnitude_re_lu_2  (None, 30, 4, 64)        1         \n  (PruneLowMagnitude)                                            \n                                                                 \n prune_low_magnitude_global_  (None, 64)               1         \n average_pooling2d (PruneLow                                     \n Magnitude)                                                      \n                                                                 \n prune_low_magnitude_dense (  (None, 2)                260       \n PruneLowMagnitude)                                              \n                                                                 \n prune_low_magnitude_softmax  (None, 2)                1         \n  (PruneLowMagnitude)                                            \n                                                                 \n=================================================================\nTotal params: 19,732\nTrainable params: 10,434\nNon-trainable params: 9,298\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"The number of non-trainable params increase since they are parameters introduced by tensorflow to handle the pruning. They will be removed by tfmot.sparsity.keras.strip_pruning(model_for_pruning) function executed in the next cells.","metadata":{"tags":[],"cell_id":"5e3764bee8e24fa692b74933ff332bed","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"### Final sparsity check","metadata":{"tags":[],"cell_id":"33e796e8c5d544069aad25048c97cde0","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"import numpy as np\n\nfor layer in model_for_pruning.layers:\n    if isinstance(layer, tf.keras.layers.Wrapper):\n        weights = layer.trainable_weights\n    else:\n        weights = layer.weights\n    for weight in weights:        \n        weight_size = weight.numpy().size\n        zero_num = np.count_nonzero(weight == 0)\n        print(\n            f'{weight.name}: {zero_num/weight_size:.2%} sparsity ',\n            f'({zero_num}/{weight_size})',\n        )","metadata":{"tags":[],"cell_id":"f8f1778b729f41aabf7fac49645af0d3","source_hash":"944e457b","execution_start":1671274954260,"execution_millis":10,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"conv2d/kernel:0: 69.97% sparsity  (403/576)\nbatch_normalization/gamma:0: 0.00% sparsity  (0/64)\nbatch_normalization/beta:0: 0.00% sparsity  (0/64)\ndepthwise_conv2d/depthwise_kernel:0: 0.00% sparsity  (0/576)\nconv2d_1/kernel:0: 69.97% sparsity  (2866/4096)\nbatch_normalization_1/gamma:0: 0.00% sparsity  (0/64)\nbatch_normalization_1/beta:0: 0.00% sparsity  (0/64)\ndepthwise_conv2d_1/depthwise_kernel:0: 0.00% sparsity  (0/576)\nconv2d_2/kernel:0: 69.97% sparsity  (2866/4096)\nbatch_normalization_2/gamma:0: 0.00% sparsity  (0/64)\nbatch_normalization_2/beta:0: 0.00% sparsity  (0/64)\ndense/kernel:0: 70.31% sparsity  (90/128)\ndense/bias:0: 0.00% sparsity  (0/2)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"The convolutional and the dense layers reached the final sparsity desired.","metadata":{"tags":[],"cell_id":"dccbd1fd164b4408a34959743f53090c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"### Model Testing","metadata":{"tags":[],"cell_id":"81a61e3bf58c44b2b4664d4d72ffcd62","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"test_loss, test_accuracy = model_for_pruning.evaluate(go_stop_test_ds)","metadata":{"tags":[],"cell_id":"608f7ae0c9864c51a86952066087fc62","source_hash":"f768990e","execution_start":1671275030584,"execution_millis":5499,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"80/80 [==============================] - 5s 63ms/step - loss: 0.0178 - sparse_categorical_accuracy: 0.9937\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"It seems that there is not overfitting. ","metadata":{"tags":[],"cell_id":"00800a8b919f4b20a3fada2e58a32d23","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"### Save the model","metadata":{"tags":[],"cell_id":"c44095ec0edd4d04be6c2e0415f3f451","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"from time import time\nimport os\n\ntimestamp = int(time())\n\nMODEL_NAME = f'model_{timestamp}'\n\n#Strip the model before save it\nmodel_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\nsaved_model_dir = f'./saved_models/{MODEL_NAME}'\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\nmodel_for_export.save(saved_model_dir)\n\n#Prepare the folder for tflite models\ntflite_models_dir = './tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)","metadata":{"tags":[],"cell_id":"5df6a41fb29547a5bbb60d2e6cb036d7","source_hash":"13c3aa8b","execution_start":1671275148312,"execution_millis":1346,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./saved_models/model_1671275148/assets\nINFO:tensorflow:Assets written to: ./saved_models/model_1671275148/assets\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### Tflite model conversion","metadata":{"tags":[],"cell_id":"743b44b0a4464f22a90e8ca1d28f7d18","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/{MODEL_NAME}')\ntflite_model = converter.convert()","metadata":{"tags":[],"cell_id":"f8f004b7fdc644b8b34e6240c03671dd","source_hash":"d18d3058","execution_start":1671275152298,"execution_millis":472,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-17 11:05:52.634891: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-17 11:05:52.634939: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-17 11:05:52.635070: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/model_1671275148\n2022-12-17 11:05:52.637517: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-17 11:05:52.637543: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./saved_models/model_1671275148\n2022-12-17 11:05:52.643528: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-17 11:05:52.673093: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./saved_models/model_1671275148\n2022-12-17 11:05:52.683054: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 47985 microseconds.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"tflite_models_dir = './tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)","metadata":{"tags":[],"cell_id":"92f3e717c13b495e8ebaa44803745957","source_hash":"5eb9fb99","execution_start":1671275156587,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":30},{"cell_type":"code","source":"tflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\ntflite_model_name","metadata":{"tags":[],"cell_id":"01e628a94df2415b9eb4c8ae23a39c4e","source_hash":"9658f457","execution_start":1671275158865,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"'./tflite_models/model_1671275148.tflite'"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"#Save the tflite model\nwith open(tflite_model_name, 'wb') as fp:\n    fp.write(tflite_model)","metadata":{"tags":[],"cell_id":"a3cca50d310440c28f7ae9c7de7510f7","source_hash":"1d5d631b","execution_start":1671275161031,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":32},{"cell_type":"code","source":"#Zip the tflite model\nimport zipfile\n\nwith zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(tflite_model_name)","metadata":{"tags":[],"cell_id":"deb62854f44d44a8b1e9e18b9181a42c","source_hash":"1e64cc35","execution_start":1671275163561,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":33},{"cell_type":"code","source":"#Check the size\ntflite_size = os.path.getsize(tflite_model_name) / 1024.0\nzipped_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024.0\n\nprint(f'Original tflite size (pruned model): {tflite_size:.3f} KB')\nprint(f'Zipped tflite size (pruned model): {zipped_size:.3f} KB')","metadata":{"tags":[],"cell_id":"27cdbcb16fef467689f75f767c88bcf2","source_hash":"4eb3df8c","execution_start":1671275171364,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Original tflite size (pruned model): 44.020 KB\nZipped tflite size (pruned model): 20.626 KB\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"The memory constraint (<25KB) is reached.","metadata":{"tags":[],"cell_id":"3f821ede1bb042b2878654560e627292","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"### Test the tflite model","metadata":{"tags":[],"cell_id":"7ddc9de5d47d484da2faf81fecf235af","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"model_path = tflite_model_name\n#------------Get the model-----------------\ninterpreter = tf.lite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nprint(\"Input name:\", input_details[0]['name'])\nprint(\"Input shape:\", input_details[0]['shape'])\nprint(\"Output name:\", output_details[0]['name'])\nprint(\"Output shape:\", output_details[0]['shape'])","metadata":{"tags":[],"cell_id":"65a70f88ad6f41a1a3db9e3ea2ed86c5","source_hash":"3618fc02","execution_start":1671275176347,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Input name: serving_default_input_1:0\nInput shape: [ 1 62 10  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"#------------File preparation------------------\nfilenames = glob('msc-test/go*') + glob('msc-test/stop*')\n\n#PREPROCESSING_MFCCS_ARGS is the one defined in the Hyperparameters section\n\n#-----------Compute the linear to mel weight matrix--------------------\ndownsampling_rate = PREPROCESSING_MFCCS_ARGS['downsampling_rate']\nsampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\nframe_length = int(downsampling_rate * PREPROCESSING_MFCCS_ARGS['frame_length_in_s'])\nframe_step = int(downsampling_rate * PREPROCESSING_MFCCS_ARGS['frame_step_in_s'])\nspectrogram_width = (16000 - frame_length) // frame_step + 1\nnum_spectrogram_bins = frame_length // 2 + 1\nnum_mfccs_coefficients = PREPROCESSING_MFCCS_ARGS['num_mfccs_coefficients']\n\nlinear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n    PREPROCESSING_MFCCS_ARGS['num_mel_bins'],\n    num_spectrogram_bins,\n    downsampling_rate,\n    PREPROCESSING_MFCCS_ARGS['lower_frequency'],\n    PREPROCESSING_MFCCS_ARGS['upper_frequency']\n)\n\n\navg_preprocessing_latency = 0.0\navg_model_latency = 0.0\nlatencies = []\naccuracy = 0.0\n#------------Test the model----------------\nfor filename in filenames:\n    audio_binary = tf.io.read_file(filename)\n    path_parts = tf.strings.split(filename, '/')\n    path_end = path_parts[-1]\n    file_parts = tf.strings.split(path_end, '_')\n    true_label = file_parts[0]\n    true_label = true_label.numpy().decode()\n    \n    start_preprocess = time()\n\n    audio, sampling_rate = tf.audio.decode_wav(audio_binary)\n    audio = tf.squeeze(audio)\n    zero_padding = tf.zeros(sampling_rate-tf.shape(audio), dtype = tf.float32) #prova a ridurre \n    audio_padded = tf.concat([audio, zero_padding], axis = 0)\n\n    stft = tf.signal.stft(\n        audio_padded,\n        frame_length = frame_length,\n        frame_step = frame_step,\n        fft_length = frame_length\n    )\n\n    spectrogram = tf.abs(stft)\n\n    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n    log_mel_spectrogram = tf.math.log(mel_spectrogram+1.e-6)\n    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n    mfccs = mfccs[:,:num_mfccs_coefficients]\n    mfccs = tf.expand_dims(mfccs, 0)\n    mfccs = tf.expand_dims(mfccs, -1)\n\n    end_preprocess = time()\n\n\n    interpreter.set_tensor(input_details[0]['index'], mfccs)\n    interpreter.invoke()\n    output = interpreter.get_tensor(output_details[0]['index'])\n\n    end_inference = time()\n\n    top_index = np.argmax(output[0])\n    predicted_label = LABELS[top_index]\n\n    accuracy += true_label == predicted_label\n    avg_preprocessing_latency += end_preprocess - start_preprocess\n    avg_model_latency += end_inference - end_preprocess\n    latencies.append(end_inference - start_preprocess)\n\n#-----------------Compute the metrics----------------------------\naccuracy /= len(filenames)\navg_preprocessing_latency /= len(filenames)\navg_model_latency /= len(filenames)\nmedian_total_latency = np.median(latencies)\nRESULTS = {\n    'accuracy': accuracy,\n    'avg_preprocessing_latency': avg_preprocessing_latency,\n    'avg_model_latency': avg_model_latency,\n    'median_total_latency': median_total_latency\n}\n\n","metadata":{"tags":[],"cell_id":"29081ed9b5cc489d882f8bfe03d77067","source_hash":"2a0d1ae4","execution_start":1671275196685,"execution_millis":2533,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":36},{"cell_type":"code","source":"RESULTS","metadata":{"tags":[],"cell_id":"9abc7a01da5a46e5855565db1ae8d7e7","source_hash":"3250ae20","execution_start":1671275202677,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"{'accuracy': 0.98,\n 'avg_preprocessing_latency': 0.005781207084655762,\n 'avg_model_latency': 0.00012733697891235352,\n 'median_total_latency': 0.00571596622467041}"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"Both accuracy and latency respect the relative constraint.","metadata":{"tags":[],"cell_id":"f6baa20eeecc46b98a3fbe3c0b4d53fc","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"### Results storage","metadata":{"tags":[],"cell_id":"1e324c693a7f4b25b21f974b354dee95","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"code","source":"import pandas as pd\n\n\noutput_dict = {\n    'tag':'Final',\n    'model_name': MODEL_NAME,\n    **PREPROCESSING_MFCCS_ARGS,\n    **TRAINING_ARGS,\n    'alpha': alpha,\n    'final_sparsity': final_sparsity,\n    'test_accuracy': test_accuracy,\n    **RESULTS,\n    'tflite_size':tflite_size,\n    'zipped_size':zipped_size \n}\n\ndf = pd.DataFrame([output_dict])\n\noutput_path=f'./team08_hw2_results.csv'\ndf.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)","metadata":{"tags":[],"cell_id":"3ad0c8083de346fdac2696daa7edb504","source_hash":"be821391","execution_start":1671275271764,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":38},{"cell_type":"code","source":"output_dict","metadata":{"tags":[],"cell_id":"ea06748e0e794106a18023048817a6c4","source_hash":"7d587d11","execution_start":1671275330697,"execution_millis":7,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"{'tag': 'Final',\n 'model_name': 'model_1671275148',\n 'downsampling_rate': 16000,\n 'frame_length_in_s': 0.016,\n 'frame_step_in_s': 0.016,\n 'num_mel_bins': 40,\n 'lower_frequency': 20,\n 'upper_frequency': 8000,\n 'num_mfccs_coefficients': 10,\n 'batch_size': 20,\n 'initial_learning_rate': 0.01,\n 'end_learning_rate': 1e-05,\n 'epochs': 20,\n 'alpha': 0.25,\n 'final_sparsity': 0.7,\n 'test_accuracy': 0.9937499761581421,\n 'accuracy': 0.98,\n 'avg_preprocessing_latency': 0.005781207084655762,\n 'avg_model_latency': 0.00012733697891235352,\n 'median_total_latency': 0.00571596622467041,\n 'tflite_size': 44.01953125,\n 'zipped_size': 20.6259765625}"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=81f45795-723f-4142-b302-f9037f9c1cf7' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"bfc877cec57f4aba82e19596571956da","deepnote_execution_queue":[]}}